"""
XXX add header doc to explain what this example does
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from pyriemann.datasets import make_gaussian_blobs
from pyriemann.embedding import SpectralEmbedding
from pyriemann.classification import MDM
from pyriemann.utils.mean import mean_riemann
from pyriemann.utils.base import invsqrtm, sqrtm, powm

from pyriemann.transferlearning_meta import TLSplitter, TLPipeline, DCT, RCT


def make_example_dataset(N, theta, alpha, eps=3.0):
    """
    N : XXX
    theta : angle for the rotation matrix applied to dataset 2
    alpha : proxy of how far the mean of dataset 2 should be from dataset 1
        XXX add a note for what value makes them aligned
    eps : XXX
    """

    # create a large set of matrices distributed in the same way
    X, y = make_gaussian_blobs(n_matrices=2*N, class_sep=eps)
    X1, y1 = X[:2*N], y[:2*N]
    X2, y2 = X[2*N:], y[2*N:]

    # we will now do some manipulations over X2 to create a dataset shift
    M2 = mean_riemann(X2)
    invsqrtM2 = invsqrtm(M2)
    # re-center dataset to identity
    X2 = invsqrtM2 @ X2 @ invsqrtM2.T
    # rotate dataset with Q
    Q = np.array(
        [[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
    X2 = Q @ X2 @ Q.T
    # transport the dataset to another place
    M1 = mean_riemann(X1)
    R = powm(M1, alpha=alpha)
    sqrtR = sqrtm(R)
    X2 = sqrtR @ X2 @ sqrtR.T

    X = np.concatenate([X1, X2])
    y = np.concatenate([y1, y2])

    meta = pd.DataFrame()
    meta['domain'] = len(X1)*[1] + len(X2)*[2]
    meta['target'] = meta['domain'] == 2  # fix domain 2 as target

    return X, y, meta


def make_figure(X, y, meta):
    """Plot the spectral embedding of the both datasets together"""

    sel1 = meta['domain'] == 1
    sel2 = meta['domain'] == 2

    y1 = y[sel1]
    y2 = y[sel2]

    N1 = np.sum(sel1)
    N2 = np.sum(sel2)

    emb = SpectralEmbedding(n_components=2, metric='riemann')
    S = emb.fit_transform(X)
    S1 = S[sel1]
    S2 = S[sel2]

    fig, ax = plt.subplots(figsize=(7.3, 6.6))
    y = np.concatenate([y1, y2])
    for i in range(N1):
        if y1[i] == 0:
            ax.scatter(S1[i, 0], S1[i, 1], c='C0', s=40, marker='x')
        if y1[i] == 1:
            ax.scatter(S1[i, 0], S1[i, 1], c='C1', s=40, marker='x')
    for i in range(N2):
        if y2[i] == 0:
            ax.scatter(S2[i, 0], S2[i, 1], c='C0', s=40, alpha=0.5, marker='o')
        if y2[i] == 1:
            ax.scatter(S2[i, 0], S2[i, 1], c='C1', s=40, alpha=0.5, marker='o')

    ax.set_xlim([-1.05, +1.05])
    ax.set_ylim([-1.05, +1.05])

    return fig


np.random.seed(13)

# create a 2D dataset with two domains, each with two classes
# both datasets are generated by the same generative procedure (gaussian blobs)
# and then one of them is transformed by a matrix A, i.e. X2 <- A @ X2 @ A.T
# with A = QP, where Q is an orthogonal matrix and P a positive definite matrix
# parameter theta defines the angle of rotation for Q and alpha is a proxy
# for how far from the Identity the mean of the new dataset should be
X, y, meta = make_example_dataset(N=50, theta=np.pi/4, alpha=5, eps=1.5)

# plot a figure with the joint spectral embedding of the simulated data points
fig = make_figure(X, y, meta)
fig.show()

# use the MDM classifier
clf = MDM()

# new object for splitting the datasets into training-validation
# the training set is composed of all data points from the source domain
# plus a partition of the target domain
target_train_frac = 0.25  # proportion of the target domain for training
n_splits = 5  # how many times to split the target domain into train/test
cv = TLSplitter(n_splits=n_splits, target_train_frac=target_train_frac)

# we consider two types of pipeline for transfer learning
# DCT : no transformation of dataset between the domains
# RCT : re-center the data points from each domain to the Identity
scores = {meth: [] for meth in ['DCT', 'RCT']}

# there are three modes for training the pipeline:
# (1) train clf only on source domain + training partition from target
# (2) train clf only on source domain
# (3) train clf only on training partition from target
# these different choices yield very different results
training_mode = 1

# carry out the cross-validation
for train_idx, test_idx in cv.split(X, y, meta):

    # split the dataset into training and testing
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    meta_train, meta_test = meta.iloc[train_idx], meta.iloc[test_idx]

    if training_mode == 2:
        y_train[meta_train["target"]] = -1
    elif training_mode == 3:
        y_train[~meta_train["target"]] = -1

    # DCT pipeline -- no transfer learning at all between source and target
    dct_transf = DCT()
    dct_pipeline = TLPipeline(transformer=dct_transf, clf=MDM())
    dct_pipeline.fit(X_train, y_train, meta_train)
    scores['DCT'].append(dct_pipeline.score(X_test, y_test, meta_test))

    # RCT pipeline -- recenter the data from each domain to identity
    rct_transf = RCT()
    rct_pipeline = TLPipeline(transformer=rct_transf, clf=MDM())
    rct_pipeline.fit(X_train, y_train, meta_train)
    scores['RCT'].append(rct_pipeline.score(X_test, y_test, meta_test))

# get the average score of each pipeline
for meth in scores.keys():
    scores[meth] = np.mean(scores[meth])
print(scores)
