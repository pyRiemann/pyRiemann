name: Deploy GitHub pages

on: [push, pull_request]

jobs:
  build_docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Restore cached datasets to avoid re-downloading from Zenodo
      - name: Restore cached datasets
        id: cache-datasets
        uses: actions/cache/restore@v4
        with:
          path: |
            ~/pyriemann_data
            ~/.mne
          key: docs-${{ runner.os }}-pyriemann-data-v1

      # Pre-download datasets when cache is cold
      - name: Pre-download datasets (cold cache)
        if: steps.cache-datasets.outputs.cache-hit != 'true'
        run: |
          pip install -e .
          python -c "
          import os
          import time
          from urllib.request import urlretrieve

          data_path = os.path.expanduser('~/pyriemann_data')
          os.makedirs(data_path, exist_ok=True)

          datasets = [
              # Salinas
              ('https://zenodo.org/records/15771735/files/Salinas.mat?download=1', 'Salinas.mat'),
              ('https://zenodo.org/records/15771735/files/Salinas_corrected.mat?download=1', 'Salinas_corrected.mat'),
              ('https://zenodo.org/records/15771735/files/Salinas_gt.mat?download=1', 'Salinas_gt.mat'),
              # UAVSAR
              ('https://zenodo.org/records/10625505/files/scene1.npy?download=1', 'scene1.npy'),
              ('https://zenodo.org/records/10625505/files/scene2.npy?download=1', 'scene2.npy'),
              # fNIRS
              ('https://zenodo.org/records/13841869/files/X.npy?download=1', 'X_fnirs.npy'),
              ('https://zenodo.org/records/13841869/files/y.npy?download=1', 'y_fnirs.npy'),
          ]

          for url, filename in datasets:
              dst = os.path.join(data_path, filename)
              if not os.path.exists(dst):
                  print(f'Downloading {filename}...')
                  urlretrieve(url, dst)
                  time.sleep(30)  # Delay to avoid Zenodo rate limiting
          print('All datasets downloaded!')
          "

      # Save cache after downloading (only on cache miss)
      - name: Save datasets cache
        if: steps.cache-datasets.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: |
            ~/pyriemann_data
            ~/.mne
          key: docs-${{ runner.os }}-pyriemann-data-v1

      - name: Generate HTML docs
        uses: gcattan/sphinx-action@master
        with:
          docs-folder: "doc/"
          pre-build-command: |
            apt-get update
            pip install -e .
            pip install -r doc/requirements.txt
        env:
          PYRIEMANN_DATA_PATH: /github/home/pyriemann_data
          SPHINX_GALLERY_PARALLEL: ${{ steps.cache-datasets.outputs.cache-hit == 'true' }}

      - name:  Upload generated HTML as artifact
        uses: actions/upload-artifact@v4
        with:
          name: DocHTML
          path: doc/build/html/

  # deploy_docs:
  #   if: github.ref == 'refs/heads/master'
  #   needs:
  #     build_docs
  #   runs-on: ubuntu-latest
  #   steps:
  #     - uses: actions/checkout@v2
  #     - name: Download artifacts
  #       uses: actions/download-artifact@v2
  #       with:
  #         name: DocHTML
  #         path: doc/build/html/
  #     - name: Commit to documentation branch
  #       run: |
  #         git clone --no-checkout --depth 1 https://github.com/${{ github.repository_owner }}/qndiag.git --branch gh-pages --single-branch gh-pages
  #         cp -r doc/build/html/* gh-pages/
  #         cd gh-pages
  #         touch .nojekyll
  #         git config --local user.email "pyriemann@github.com"
  #         git config --local user.name "pyriemann GitHub Action"
  #         git add .
  #         git commit -m "Update documentation" -a || true
  #     - name: Push changes
  #       uses: ad-m/github-push-action@v0.6.0
  #       with:
  #         branch: gh-pages
  #         directory: gh-pages
  #         github_token: ${{ secrets.GITHUB_TOKEN }}
