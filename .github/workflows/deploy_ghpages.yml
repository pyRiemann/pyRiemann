name: Deploy GitHub pages

on: [push, pull_request]

jobs:
  build_docs:
    runs-on: ubuntu-latest
    env:
      PYRIEMANN_DATA_PATH: ${{ github.workspace }}/pyriemann_data
      MNE_DATA: ${{ github.workspace }}/mne_data
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Restore cached datasets to avoid re-downloading from Zenodo
      - name: Restore cached datasets
        id: cache-datasets
        uses: actions/cache/restore@v4
        with:
          path: |
            pyriemann_data
            mne_data
          key: docs-${{ runner.os }}-pyriemann-data-v4

      # Pre-download datasets when cache is cold
      - name: Pre-download datasets (cold cache)
        if: steps.cache-datasets.outputs.cache-hit != 'true'
        run: |
          pip install -e .
          pip install mne
          python -c "
          import os
          import time
          from urllib.request import urlretrieve
          from pyriemann.utils._data import get_data_path

          # fNIRS data
          fnirs_path = get_data_path('fnirs')
          os.makedirs(fnirs_path, exist_ok=True)
          fnirs_files = [
              ('https://zenodo.org/records/13841869/files/X.npy?download=1', 'X.npy'),
              ('https://zenodo.org/records/13841869/files/y.npy?download=1', 'y.npy'),
          ]

          # image-radar data
          salinas_path = get_data_path('salinas')
          os.makedirs(salinas_path, exist_ok=True)
          uavsar_path = get_data_path('uavsar')
          os.makedirs(uavsar_path, exist_ok=True)
          salinas_files = [
              ('https://zenodo.org/records/15771735/files/Salinas.mat?download=1', 'Salinas.mat'),
              ('https://zenodo.org/records/15771735/files/Salinas_corrected.mat?download=1', 'Salinas_corrected.mat'),
              ('https://zenodo.org/records/15771735/files/Salinas_gt.mat?download=1', 'Salinas_gt.mat'),
          ]
          uavsar_files = [
              ('https://zenodo.org/records/10625505/files/scene1.npy?download=1', 'scene1.npy'),
              ('https://zenodo.org/records/10625505/files/scene2.npy?download=1', 'scene2.npy'),
          ]

          # Download all files with delays
          for path, files in [
              (fnirs_path, fnirs_files),
              (salinas_path, salinas_files),
              (uavsar_path, uavsar_files),
          ]:
              for url, filename in files:
                  dst = os.path.join(path, filename)
                  if not os.path.exists(dst):
                      print(f'Downloading {filename} to {path}...')
                      urlretrieve(url, dst)
                      time.sleep(30)  # Delay to avoid Zenodo rate limiting
          print('All datasets downloaded!')

          # MNE datasets (eegbci, sample, and SSVEP)
          import mne
          from mne.datasets import eegbci, sample, fetch_dataset
          mne.set_config('MNE_DATA', os.environ['MNE_DATA'], set_env=False)
          os.makedirs(os.environ['MNE_DATA'], exist_ok=True)

          # Download eegbci data
          for subject, runs in {1: [6, 10], 2: [5], 7: [4, 6, 8, 10, 14]}.items():
              eegbci.load_data(subject, runs, update_path=True)

          # Download MNE sample data
          sample.data_path(download=True, update_path=True)

          # Download SSVEP data via MNE fetch_dataset
          ssvep_url = 'https://zenodo.org/record/2392979/files/'
          for fname in ['subject12_run2_raw.fif', 'subject12_run5_raw.fif']:
              url = f'{ssvep_url}{fname}'
              archive_name = url.split('/')[-4:]
              fetch_dataset({
                  'dataset_name': 'ssvep',
                  'archive_name': '/'.join(archive_name),
                  'hash': None,
                  'url': url,
                  'folder_name': 'MNE-ssvepexo-data',
                  'config_key': 'MNE_DATASETS_SSVEPEXO_PATH',
              }, force_update=True)
          print('All MNE datasets downloaded!')
          "

      # Save cache after downloading (only on cache miss)
      - name: Save datasets cache
        if: steps.cache-datasets.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: |
            pyriemann_data
            mne_data
          key: docs-${{ runner.os }}-pyriemann-data-v4

      - name: Generate HTML docs
        uses: gcattan/sphinx-action@master
        with:
          docs-folder: "doc/"
          pre-build-command: |
            apt-get update
            pip install -e .
            pip install -r doc/requirements.txt
        env:
          PYRIEMANN_DATA_PATH: /github/workspace/pyriemann_data
          MNE_DATA: /github/workspace/mne_data
          SPHINX_GALLERY_PARALLEL: ${{ steps.cache-datasets.outputs.cache-hit == 'true' }}

      - name:  Upload generated HTML as artifact
        uses: actions/upload-artifact@v4
        with:
          name: DocHTML
          path: doc/build/html/

  # deploy_docs:
  #   if: github.ref == 'refs/heads/master'
  #   needs:
  #     build_docs
  #   runs-on: ubuntu-latest
  #   steps:
  #     - uses: actions/checkout@v2
  #     - name: Download artifacts
  #       uses: actions/download-artifact@v2
  #       with:
  #         name: DocHTML
  #         path: doc/build/html/
  #     - name: Commit to documentation branch
  #       run: |
  #         git clone --no-checkout --depth 1 https://github.com/${{ github.repository_owner }}/qndiag.git --branch gh-pages --single-branch gh-pages
  #         cp -r doc/build/html/* gh-pages/
  #         cd gh-pages
  #         touch .nojekyll
  #         git config --local user.email "pyriemann@github.com"
  #         git config --local user.name "pyriemann GitHub Action"
  #         git add .
  #         git commit -m "Update documentation" -a || true
  #     - name: Push changes
  #       uses: ad-m/github-push-action@v0.6.0
  #       with:
  #         branch: gh-pages
  #         directory: gh-pages
  #         github_token: ${{ secrets.GITHUB_TOKEN }}
